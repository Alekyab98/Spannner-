import csv
import argparse
from datetime import date, timedelta
from google.cloud import spanner

def get_dates_in_range(start_date_str, end_date_str):
    try:
        start_date = date.fromisoformat(start_date_str)
        end_date = date.fromisoformat(end_date_str)
    except ValueError:
        raise ValueError("Please provide dates in YYYY-MM-DD format.")

    dates = []
    current_date = start_date
    while current_date <= end_date:
        dates.append(current_date.strftime('%Y-%m-%d'))
        current_date += timedelta(days=1)
    return dates


def check_spanner_data_batch(project, instance_id, db_name, table_name, filter_col, date_list):
    """
    Executes a single Spanner query using UNION ALL to check all dates at once.
    Returns a dictionary: {date: 'Yes'/'No'}
    """
    spanner_client = spanner.Client(project=project)
    instance = spanner_client.instance(instance_id)
    database = instance.database(db_name)

    union_query = "\nUNION ALL\n".join([
        f"SELECT '{dt}' AS check_date, EXISTS (SELECT 1 FROM {table_name} WHERE date(tiemstamp(<filter_col>),'utc') = '{dt}' LIMIT 1) AS has_data"
        for dt in date_list
    ])

    result_map = {}
    try:
        with database.snapshot() as snapshot:
            result_set = snapshot.execute_sql(union_query)
            for row in result_set:
                dt, has_data = row
                result_map[dt] = 'Yes' if has_data else 'No'
    except Exception as e:
        print(f"‚ùå Error querying {db_name}.{table_name}: {e}")
        for dt in date_list:
            result_map[dt] = 'Error'
    return result_map


def main():
    parser = argparse.ArgumentParser(description="Check for data availability in Spanner tables.")
    parser.add_argument("input_csv", help="Path to the input CSV file.")
    parser.add_argument("start_date", help="Start date in YYYY-MM-DD format.")
    parser.add_argument("end_date", help="End date in YYYY-MM-DD format.")
    parser.add_argument("instance_id", help="Google Cloud Spanner instance ID.")
    parser.add_argument("project_id", help="Google Cloud Spanner project ID.")
    args = parser.parse_args()

    output_csv = 'spanner_data_availability_output.csv'
    all_dates = get_dates_in_range(args.start_date, args.end_date)

    with open(args.input_csv, 'r') as infile, open(output_csv, 'w', newline='') as outfile:
        reader = csv.DictReader(infile)
        fieldnames = reader.fieldnames + ['date', 'data_available']
        writer = csv.DictWriter(outfile, fieldnames=fieldnames)
        writer.writeheader()

        for row in reader:
            db_name = row['db_name']
            table_name = row['table_name']
            filter_col = row['filter_col']

            print(f"üìå Checking {db_name}.{table_name} from {args.start_date} to {args.end_date}...")

            results = check_spanner_data_batch(
                args.project_id,
                args.instance_id,
                db_name,
                table_name,
                filter_col,
                all_dates
            )

            for single_date in all_dates:
                output_row = row.copy()
                output_row['date'] = single_date
                output_row['data_available'] = results.get(single_date, 'Error')
                writer.writerow(output_row)

    print(f"\n‚úÖ Done! Results saved to: {output_csv}")


if __name__ == '__main__':
    main()
